{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprised-priest",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crazy-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "# Pretty Printer\n",
    "from pprint import pprint\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Numpy\n",
    "import numpy as np\n",
    "# Pillow\n",
    "from PIL import Image\n",
    "# Torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "# Models\n",
    "from model import healthy_model, covid_model, save_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-festival",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-answer",
   "metadata": {},
   "source": [
    "Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plain-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-projection",
   "metadata": {},
   "source": [
    "The dataset has been split into training, validation, and testing sets, containing multiple non-identical pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mounted-antique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'val', 'test']\n"
     ]
    }
   ],
   "source": [
    "groups = ['train', 'val', 'test']\n",
    "pprint(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-observation",
   "metadata": {},
   "source": [
    "The images in the dataset are X-ray pictures of either infected lungs (class index 1 and label 'infected') or non-infected lungs (class index 0 and label 'normal'). We will use this classification for our first binary classifier (normal or infected lungs) and we call this the Initial Model.\n",
    "\n",
    "For images in the infected sub-dataset, X-ray pictures can either be of COVID-infected lungs (class index 0 and label 'covid') or any other infected lungs (class index 1 and label 'non_covid'). We will use this classificatino for our second binary classifier (COVID or Non-Covid infected lungs) and we call this the Infected Model.\n",
    "\n",
    "For testing, we will use the combined classes to classify the X-ray pictures. We first input the image into the Initial Model. If the output label is 'normal', we don't need to pass this image into our Infected Model. However, if the output label is 'infected', we pass this image into the Infected Model to further classify whether it is infected by COVID or non_covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nearby-spank",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model classes: {0: 'normal', 1: 'infected'}\n",
      "\n",
      "Infected Model classes: {0: 'covid', 1: 'non_covid'}\n",
      "\n",
      "Combined Model classes: {0: 'normal', 1: 'covid', 2: 'non_covid'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_classes = {0: \"normal\", 1: \"infected\"}\n",
    "infected_classes = {0: \"covid\", 1: \"non_covid\"}\n",
    "combined_classes = {0: \"normal\", 1: \"covid\", 2: \"non_covid\"}\n",
    "print(\"Initial Model classes: {}\\n\".format(initial_classes))\n",
    "print(\"Infected Model classes: {}\\n\".format(infected_classes))\n",
    "print(\"Combined Model classes: {}\\n\".format(combined_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-currency",
   "metadata": {},
   "source": [
    "Each subset is stored in a different subfolder of the **./dataset/** folder, as listed in the dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wound-tracy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_infected_covid': './dataset/test/infected/covid',\n",
      " 'test_infected_non_covid': './dataset/test/infected/non-covid',\n",
      " 'test_normal': './dataset/test/normal/',\n",
      " 'train_infected_covid': './dataset/train/infected/covid',\n",
      " 'train_infected_non_covid': './dataset/train/infected/non-covid',\n",
      " 'train_normal': './dataset/train/normal/',\n",
      " 'val_infected_covid': './dataset/val/infected/covid',\n",
      " 'val_infected_non_covid': './dataset/val/infected/non-covid',\n",
      " 'val_normal': './dataset/val/normal/'}\n"
     ]
    }
   ],
   "source": [
    "# Path to images for different parts of the dataset\n",
    "dataset_paths = {'train_normal': './dataset/train/normal/',\n",
    "                 'train_infected_covid': './dataset/train/infected/covid',\n",
    "                 'train_infected_non_covid': './dataset/train/infected/non-covid',\n",
    "                 'val_normal': './dataset/val/normal/',\n",
    "                 'val_infected_covid': './dataset/val/infected/covid',\n",
    "                 'val_infected_non_covid': './dataset/val/infected/non-covid',\n",
    "                 'test_normal': './dataset/test/normal/',\n",
    "                 'test_infected_covid': './dataset/test/infected/covid',\n",
    "                 'test_infected_non_covid': './dataset/test/infected/non-covid'}\n",
    "pprint(dataset_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-parker",
   "metadata": {},
   "source": [
    "We can count the numbers of samples (X-ray pictures) inside each of the subfolders and store them in a dictionary ***dataset_numbers***. Note that an infected lung means it is infected by either COVID or other non-COVID diseases, thus we also calculate a tally of infected X-ray lung images (COVID infected + non-COVID infected). This will be useful for our binary classifier later that classifies either a normal lung (non-infected) or an infected lung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-retreat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_infected': 381,\n",
      " 'test_infected_covid': 139,\n",
      " 'test_infected_non_covid': 242,\n",
      " 'test_normal': 234,\n",
      " 'test_total': 615,\n",
      " 'train_infected': 3875,\n",
      " 'train_infected_covid': 1345,\n",
      " 'train_infected_non_covid': 2530,\n",
      " 'train_normal': 1341,\n",
      " 'train_total': 5216,\n",
      " 'val_infected': 17,\n",
      " 'val_infected_covid': 9,\n",
      " 'val_infected_non_covid': 8,\n",
      " 'val_normal': 8,\n",
      " 'val_total': 25}\n"
     ]
    }
   ],
   "source": [
    "dataset_numbers = {}\n",
    "\n",
    "for key, value in dataset_paths.items():\n",
    "    dataset_numbers[key] = len(os.listdir(value))\n",
    "\n",
    "for group in groups:\n",
    "    dataset_numbers[\"{}_infected\".format(group)] = dataset_numbers['{}_infected_covid'.format(group)] + dataset_numbers['{}_infected_non_covid'.format(group)]\n",
    "    dataset_numbers[\"{}_total\".format(group)] = dataset_numbers[\"{}_infected\".format(group)] + dataset_numbers[\"{}_normal\".format(group)]\n",
    "\n",
    "pprint(dataset_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-click",
   "metadata": {},
   "source": [
    "A helper function to get relevant attributes from the dataset. The attribute can only be dataset_numbers or dataset_paths, while the group can only be 'train', 'val', or 'test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liable-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [dataset_numbers, dataset_paths]\n",
    "\n",
    "def get_dataset_attributes(group, attribute, infected_only = False):\n",
    "    assert group in groups, \"Invalid group input. Group can only be 'train', 'val', or 'test'\"\n",
    "    assert attribute in attributes, \"Invalid attribute. Attribute can only be dataset_numbers or dataset_paths\"\n",
    "    res = {}\n",
    "    for key, value in attribute.items():\n",
    "        if group in key:\n",
    "            if not infected_only:\n",
    "                res[key] = value\n",
    "            elif 'infected' in key:\n",
    "                res[key] = value\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-tuesday",
   "metadata": {},
   "source": [
    "## 3. Create a Dataset Generator that is reusable to generate Training Dataset and Validating Dataset of Initial Model (First Binary Classifier).\n",
    "\n",
    "The Dataset generated will contain two special methods, which need to be defined if we want to transform our Dataset object into a dataloader later on.\n",
    "\n",
    "The first method is the special length method ( __ len __ ), which should return the number of images present in the dataset. We can easily obtain this from ```dataset_numbers[{SELECTED_GROUP}_total]``` (e.g. ```dataset_numbers[train_total]``` or ```dataset_numbers[val_total]```.\n",
    "\n",
    "The second one is the special getitem method ( __ getitem __ ), which is used to fetch an image and its label, using a single index value.\n",
    "\n",
    "By convention here, we have decided to open the image in ```./dataset/{SELECTED_GROUP}/normal/XXX.jpg```, if the value in XXX is strictly lower than the total number of normal lung images in the selected group's dataset.\n",
    "\n",
    "Otherwise, we will open the image somewhere in ```./dataset/{SELECTED_GROUP}/infected/```. Again, by convention, we will open the image in ```./dataset_demo/{SELECTED_GROUP}/infected/covid/YYY.jpg``` if YYY is strictly lower than ```(total COVID images) - XXX```.\n",
    "\n",
    "Otherwise, we open the image in ```./dataset/{SELECTED_GROUP}/infected/non-covid/ZZZ.jpg```, where ZZZ is simply defined as ZZZ = YYY - total COVID images.\n",
    "\n",
    "For example, let's say we supply ***__getitem__*** with ***index*** = **5000** in the **Training Dataset**. There are 1341 images of normal lungs in ```./dataset/train/normal/```. Since 5000 > 1341, we will have to open the image somewhere in ```./dataset/train/infected/```.\n",
    "\n",
    "There are 1345 images of COVID-infected lungs in ```./dataset_demo/train/infected/covid/```. Since 5000 - 1341 > 1345, this means we will have to take the image from ```./dataset_demo/train/infected/non-covid/```. We will thus take image with index 5000 - 1341 - 1345 = **2314.jpg** in ```./dataset_demo/train/infected/non-covid/```.\n",
    "\n",
    "Note that if the value in XXX is larger than the length of the dataset (i.e. 5216 - 1 = 5215), then no image will be opened and the getitem will display an error, based on the asserts we have in the ***open_img*** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "limited-sociology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Lung_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, group):\n",
    "        \"\"\"\n",
    "        Construtor for Training Dataset class\n",
    "        Picks important parameters in attributes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # All images are of size 150 x 150\n",
    "        self.img_size = (150, 150)\n",
    "        \n",
    "        # Only two classes will be considered here (normal and infected)\n",
    "        self.classes = initial_classes\n",
    "        \n",
    "        # The dataset consists only of images from the specified group\n",
    "        self.groups = group\n",
    "        \n",
    "        # Total Aggregator\n",
    "        self.aggregator = \"total\"\n",
    "        \n",
    "        # Number of images in each part\n",
    "        self.dataset_numbers = get_dataset_attributes(group, dataset_numbers)\n",
    "        \n",
    "        # Path to images for different parts of the dataset\n",
    "        self.dataset_paths = get_dataset_attributes(group, dataset_paths)\n",
    "    \n",
    "    def describe(self):\n",
    "        \"\"\"\n",
    "        Descriptor function.\n",
    "        Will print details about the dataset when called.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate description\n",
    "        msg = \"This is the {}ing dataset derived from the Lung Dataset\".format('validat' if self.groups == 'val' else self.groups)\n",
    "        msg += \" used for the Small Project in the 50.039 Deep Learning class\"\n",
    "        msg += \" in Feb - March 2021.\\n\"\n",
    "        msg += \"It contains a total of {} images, \".format(self.dataset_numbers[\"{}_{}\".format(self.groups, self.aggregator)])\n",
    "        msg += \"of size {} by {}.\\n\".format(self.img_size[0], self.img_size[1])\n",
    "        msg += \"The images are stored in the following locations \"\n",
    "        msg += \"and each one contains the following number of images:\\n\"\n",
    "        for key, value in self.dataset_paths.items():\n",
    "            msg += \" - {}, in folder {}: {} images.\\n\".format(key, value, self.dataset_numbers[key])\n",
    "        print(msg)\n",
    "        \n",
    "    def open_img(self, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Asserts checking for consistency in passed parameters        \n",
    "        err_msg = \"Error - class_val variable should be set to 'normal' or 'infected'.\"\n",
    "        assert class_val in self.classes.values(), err_msg\n",
    "        \n",
    "        max_val = self.dataset_numbers['{}_{}'.format(self.groups, class_val)]\n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        err_msg += \"\\n(In {}/{}, you have {} images.)\".format(self.groups, class_val, max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val < max_val, err_msg\n",
    "        \n",
    "        # Open file as before\n",
    "        covid_total = dataset_numbers['{}_infected_covid'.format(self.groups)]\n",
    "        if class_val == 'infected' and index_val < covid_total:\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_infected_covid'.format(self.groups)], index_val)\n",
    "        elif class_val == 'infected':\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_infected_non_covid'.format(self.groups)], index_val - covid_total)\n",
    "        else:\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_normal'.format(self.groups)], index_val)\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    def show_img(self, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens, then displays image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - class_val variable should be set to 'normal' or 'infected'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Open image\n",
    "        im = self.open_img(class_val, index_val)\n",
    "        \n",
    "        # Display\n",
    "        plt.imshow(im)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Length special method, returns the number of images in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Length function\n",
    "        return self.dataset_numbers['{}_{}'.format(self.groups, self.aggregator)]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get item special method\n",
    "        normal_total = self.dataset_numbers[\"{}_normal\".format(self.groups)]\n",
    "        if index < normal_total:\n",
    "            class_val = 'normal'\n",
    "            label = torch.tensor(0, dtype = torch.long, device = device)\n",
    "        else:\n",
    "            class_val = 'infected'\n",
    "            index -= normal_total\n",
    "            label = torch.tensor(1, dtype = torch.long, device = device)\n",
    "        im = self.open_img(class_val, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-emperor",
   "metadata": {},
   "source": [
    "Create **Training Dataset** (for Initial Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modified-imperial",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the training dataset derived from the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb - March 2021.\n",
      "It contains a total of 5216 images, of size 150 by 150.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - train_normal, in folder ./dataset/train/normal/: 1341 images.\n",
      " - train_infected_covid, in folder ./dataset/train/infected/covid: 1345 images.\n",
      " - train_infected_non_covid, in folder ./dataset/train/infected/non-covid: 2530 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ld_train = Lung_Dataset('train')\n",
    "ld_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-remove",
   "metadata": {},
   "source": [
    "Create **Testing Dataset** (for Initial Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "level-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the testing dataset derived from the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb - March 2021.\n",
      "It contains a total of 615 images, of size 150 by 150.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - test_normal, in folder ./dataset/test/normal/: 234 images.\n",
      " - test_infected_covid, in folder ./dataset/test/infected/covid: 139 images.\n",
      " - test_infected_non_covid, in folder ./dataset/test/infected/non-covid: 242 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ld_test = Lung_Dataset('test')\n",
    "ld_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-invasion",
   "metadata": {},
   "source": [
    "## 4. Create an Infected Dataset Generator to generate Infected Dataset for training and validation of Infected Model (Second Binary Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abstract-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infected_Lung_Dataset(Lung_Dataset):\n",
    "    \n",
    "    def __init__(self, group):\n",
    "        \"\"\"\n",
    "        Construtor for Training Dataset class\n",
    "        Picks important parameters in attributes.\n",
    "        \"\"\"        \n",
    "        super().__init__(group)\n",
    "        \n",
    "        # Only two classes will be considered here (covid and non-covid)\n",
    "        self.classes = infected_classes\n",
    "        \n",
    "        # Total Aggregator\n",
    "        self.aggregator = 'infected'\n",
    "        \n",
    "        # Number of images in each part\n",
    "        self.dataset_numbers = get_dataset_attributes(group, dataset_numbers, infected_only = True)\n",
    "        \n",
    "        # Path to images for different parts of the dataset\n",
    "        self.dataset_paths = get_dataset_attributes(group, dataset_paths, infected_only = True)\n",
    "        \n",
    "    def open_img(self, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - class_val variable should be set to 'covid' or 'non_covid'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Asserts checking for consistency in passed parameters        \n",
    "        err_msg = \"Error - class_val variable should be set to 'covid' or 'non-covid'.\"\n",
    "        assert class_val in self.classes.values(), err_msg\n",
    "        \n",
    "        max_val = self.dataset_numbers['{}_infected_{}'.format(self.groups, class_val)]\n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        err_msg += \"\\n(In {}/infected/{}, you have {} images.)\".format(self.groups, class_val, max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val < max_val, err_msg\n",
    "        \n",
    "        # Open file as before\n",
    "        path_to_file = '{}/{}.jpg'.format(self.dataset_paths['{}_infected_{}'.format(self.groups, class_val)], index_val)\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get item special method\n",
    "        covid_total = self.dataset_numbers[\"{}_infected_covid\".format(self.groups)]\n",
    "        if index < covid_total:\n",
    "            class_val = 'covid'\n",
    "            label = torch.tensor(0, dtype = torch.long, device = device)\n",
    "        else:\n",
    "            class_val = 'non_covid'\n",
    "            index -= covid_total\n",
    "            label = torch.tensor(1, dtype = torch.long, device = device)\n",
    "        im = self.open_img(class_val, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-conservative",
   "metadata": {},
   "source": [
    "Create **Infected Training Dataset** (for Infected Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wicked-marshall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the training dataset derived from the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb - March 2021.\n",
      "It contains a total of 3875 images, of size 150 by 150.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - train_infected_covid, in folder ./dataset/train/infected/covid: 1345 images.\n",
      " - train_infected_non_covid, in folder ./dataset/train/infected/non-covid: 2530 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ld_train_infected = Infected_Lung_Dataset('train')\n",
    "ld_train_infected.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-glucose",
   "metadata": {},
   "source": [
    "Create **Infected Testing Dataset** (for Infected Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fixed-habitat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the testing dataset derived from the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb - March 2021.\n",
      "It contains a total of 381 images, of size 150 by 150.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - test_infected_covid, in folder ./dataset/test/infected/covid: 139 images.\n",
      " - test_infected_non_covid, in folder ./dataset/test/infected/non-covid: 242 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ld_test_infected = Infected_Lung_Dataset('test')\n",
    "ld_test_infected.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-progress",
   "metadata": {},
   "source": [
    "## 5. Create Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "medieval-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Lung_Dataset(Lung_Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Construtor for Training Dataset class\n",
    "        Picks important parameters in attributes.\n",
    "        \"\"\"\n",
    "        super().__init__('val')\n",
    "        \n",
    "        # Use combined classes (0 for normal, 1 for COVID, 2 for infected by non-COVID)\n",
    "        self.classes = combined_classes\n",
    "        \n",
    "    def open_img(self, class_val, index_val):\n",
    "        \"\"\"\n",
    "        Opens image with specified parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - class_val variable should be set to 'normal' or 'covid' or 'non_covid'.\n",
    "        - index_val should be an integer with values between 0 and the maximal number of images in dataset.\n",
    "        \n",
    "        Returns loaded image as a normalized Numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Asserts checking for consistency in passed parameters        \n",
    "        err_msg = \"Error - class_val variable should be set to 'normal' or 'covid' or 'non_covid'.\"\n",
    "        assert class_val in self.classes.values(), err_msg\n",
    "        \n",
    "        err_msg = \"Error - index_val variable should be an integer between 0 and the maximal number of images.\"\n",
    "        if 'covid' in class_val:\n",
    "            max_val = self.dataset_numbers['val_infected_{}'.format(class_val)]\n",
    "            err_msg += \"\\n(In test/infected/{}, you have {} images.)\".format(class_val, max_val)\n",
    "        else:\n",
    "            max_val = self.dataset_numbers['val_normal']\n",
    "            err_msg += \"\\n(In test/normal, you have {} images.)\".format(max_val)\n",
    "        assert isinstance(index_val, int), err_msg\n",
    "        assert index_val >= 0 and index_val < max_val, err_msg\n",
    "        \n",
    "        # Open file as before\n",
    "        if 'covid' in class_val:\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['val_infected_{}'.format(class_val)], index_val)\n",
    "        else:\n",
    "            path_to_file = '{}/{}.jpg'.format(self.dataset_paths['val_normal'], index_val)\n",
    "        with open(path_to_file, 'rb') as f:\n",
    "            im = np.asarray(Image.open(f))/255\n",
    "        f.close()\n",
    "        return im\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Getitem special method.\n",
    "        \n",
    "        Expects an integer value index, between 0 and len(self) - 1.\n",
    "        \n",
    "        Returns the image and its label as a one hot vector, both\n",
    "        in torch tensor format in dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get item special method\n",
    "        normal_total = self.dataset_numbers[\"val_normal\"]\n",
    "        covid_total = self.dataset_numbers[\"val_infected_covid\"]\n",
    "        if index < normal_total:\n",
    "            class_val = 'normal'\n",
    "            label = torch.tensor(0, dtype = torch.long, device = device)\n",
    "        else:\n",
    "            index -= normal_total\n",
    "            if index < covid_total:\n",
    "                class_val = \"covid\"\n",
    "                label = torch.tensor(1, dtype = torch.long, device = device)\n",
    "            else:\n",
    "                class_val = \"non_covid\"\n",
    "                index -= covid_total\n",
    "                label = torch.tensor(2, dtype = torch.long, device = device)\n",
    "        im = self.open_img(class_val, index)\n",
    "        im = transforms.functional.to_tensor(np.array(im)).float()\n",
    "        return im, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "another-concern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the validating dataset derived from the Lung Dataset used for the Small Project in the 50.039 Deep Learning class in Feb - March 2021.\n",
      "It contains a total of 25 images, of size 150 by 150.\n",
      "The images are stored in the following locations and each one contains the following number of images:\n",
      " - val_normal, in folder ./dataset/val/normal/: 8 images.\n",
      " - val_infected_covid, in folder ./dataset/val/infected/covid: 9 images.\n",
      " - val_infected_non_covid, in folder ./dataset/val/infected/non-covid: 8 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ld_test_final = Test_Lung_Dataset()\n",
    "ld_test_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-conflict",
   "metadata": {},
   "source": [
    "## 6. Visualization of the image distribution in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "neural-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 612x612 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "labels = [\"normal\", \"infected_covid\", \"infected_non_covid\"]\n",
    "train_bar_values = [dataset_numbers[\"train_normal\"], dataset_numbers[\"train_infected_covid\"], dataset_numbers[\"train_infected_non_covid\"]]\n",
    "val_bar_values = [dataset_numbers[\"val_normal\"], dataset_numbers[\"val_infected_covid\"], dataset_numbers[\"val_infected_non_covid\"]]\n",
    "test_bar_values = [dataset_numbers[\"test_normal\"], dataset_numbers[\"test_infected_covid\"], dataset_numbers[\"test_infected_non_covid\"]]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, train_bar_values, width, label='Train')\n",
    "rects2 = ax.bar(x, val_bar_values, width, label='Val')\n",
    "rects3 = ax.bar(x + width, test_bar_values, width, label='Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('No. of Samples')\n",
    "ax.set_title('Image Distribution in Dataset')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(8.5, 8.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-story",
   "metadata": {},
   "source": [
    "## 7. Create the Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "earlier-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size value to be used\n",
    "bs_val = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "romantic-audio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001834FB00700>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001834FB006D0>\n"
     ]
    }
   ],
   "source": [
    "# Dataloader for Training and Validation Datasets of Initial Model (First Binary Classifier)\n",
    "first_train_loader = DataLoader(ld_train, batch_size = bs_val, shuffle = True)\n",
    "first_test_loader = DataLoader(ld_test, batch_size = bs_val, shuffle = True)\n",
    "print(first_train_loader)\n",
    "print(first_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ignored-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001834FB0F6A0>\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x000001834FB0F400>\n"
     ]
    }
   ],
   "source": [
    "# Dataloader for Training and Validation Datasets of Infected Model (Second Binary Classifier)\n",
    "infected_train_loader = DataLoader(ld_train_infected, batch_size = bs_val, shuffle = True)\n",
    "infected_test_loader = DataLoader(ld_test_infected, batch_size = bs_val, shuffle = True)\n",
    "print(infected_train_loader)\n",
    "print(infected_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "japanese-interview",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001834FA46580>\n"
     ]
    }
   ],
   "source": [
    "# Dataloader for Testing Dataset\n",
    "test_loader = DataLoader(ld_test_final, batch_size = bs_val, shuffle = True)\n",
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-teens",
   "metadata": {},
   "source": [
    "## 8. RYAN PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-uncertainty",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models with 3 convolutional layers\n",
      "==============================\n",
      "\n",
      "Running Initial Model\n",
      "Net(\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=663552, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "model is training\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 1 [0/5216 (0%)]\tLoss: 0.633503\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 1 [256/5216 (5%)]\tLoss: 0.429678\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [512/5216 (10%)]\tLoss: 0.702579\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [768/5216 (15%)]\tLoss: 0.629397\n",
      "Accuracy: 0.5\n",
      "Train Epoch: 1 [1024/5216 (20%)]\tLoss: 0.732019\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [1280/5216 (25%)]\tLoss: 0.505293\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [1536/5216 (29%)]\tLoss: 0.635109\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [1792/5216 (34%)]\tLoss: 0.646339\n",
      "Accuracy: 0.5\n",
      "Train Epoch: 1 [2048/5216 (39%)]\tLoss: 0.862293\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 1 [2304/5216 (44%)]\tLoss: 0.246883\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 1 [2560/5216 (49%)]\tLoss: 0.750118\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [2816/5216 (54%)]\tLoss: 0.555489\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [3072/5216 (59%)]\tLoss: 0.515716\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 1 [3328/5216 (64%)]\tLoss: 0.460147\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [3584/5216 (69%)]\tLoss: 0.488895\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 1 [3840/5216 (74%)]\tLoss: 0.531504\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [4096/5216 (79%)]\tLoss: 0.410926\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [4352/5216 (83%)]\tLoss: 0.469780\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [4608/5216 (88%)]\tLoss: 0.268093\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [4864/5216 (93%)]\tLoss: 0.295423\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [5120/5216 (98%)]\tLoss: 0.412040\n",
      "\n",
      "Epoch 1 Validation set: Average loss: 0.0052, Accuracy: 438/615 (71%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [0/5216 (0%)]\tLoss: 0.156620\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [256/5216 (5%)]\tLoss: 0.277039\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [512/5216 (10%)]\tLoss: 0.307801\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [768/5216 (15%)]\tLoss: 0.282036\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [1024/5216 (20%)]\tLoss: 0.207615\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [1280/5216 (25%)]\tLoss: 0.467328\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 2 [1536/5216 (29%)]\tLoss: 0.340190\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 2 [1792/5216 (34%)]\tLoss: 0.171140\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [2048/5216 (39%)]\tLoss: 0.406813\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [2304/5216 (44%)]\tLoss: 0.312744\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [2560/5216 (49%)]\tLoss: 0.474093\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [2816/5216 (54%)]\tLoss: 0.555720\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 2 [3072/5216 (59%)]\tLoss: 0.178782\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 2 [3328/5216 (64%)]\tLoss: 0.080365\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 2 [3584/5216 (69%)]\tLoss: 0.047604\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [3840/5216 (74%)]\tLoss: 0.138496\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [4096/5216 (79%)]\tLoss: 0.259539\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [4352/5216 (83%)]\tLoss: 0.192949\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 2 [4608/5216 (88%)]\tLoss: 0.371203\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [4864/5216 (93%)]\tLoss: 0.124475\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [5120/5216 (98%)]\tLoss: 0.253477\n",
      "\n",
      "Epoch 2 Validation set: Average loss: 0.0051, Accuracy: 450/615 (73%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [0/5216 (0%)]\tLoss: 0.239388\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [256/5216 (5%)]\tLoss: 0.171083\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [512/5216 (10%)]\tLoss: 0.205495\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [768/5216 (15%)]\tLoss: 0.135258\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [1024/5216 (20%)]\tLoss: 0.398964\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [1280/5216 (25%)]\tLoss: 0.434362\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [1536/5216 (29%)]\tLoss: 0.180421\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [1792/5216 (34%)]\tLoss: 0.422986\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [2048/5216 (39%)]\tLoss: 0.070254\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [2304/5216 (44%)]\tLoss: 0.249065\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 3 [2560/5216 (49%)]\tLoss: 0.115831\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [2816/5216 (54%)]\tLoss: 0.284857\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [3072/5216 (59%)]\tLoss: 0.425644\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [3328/5216 (64%)]\tLoss: 0.114014\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [3584/5216 (69%)]\tLoss: 0.280114\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [3840/5216 (74%)]\tLoss: 0.170328\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 3 [4096/5216 (79%)]\tLoss: 0.047824\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 3 [4352/5216 (83%)]\tLoss: 0.119658\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [4608/5216 (88%)]\tLoss: 0.096578\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [4864/5216 (93%)]\tLoss: 0.153760\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 3 [5120/5216 (98%)]\tLoss: 0.095983\n",
      "\n",
      "Epoch 3 Validation set: Average loss: 0.0029, Accuracy: 502/615 (82%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [0/5216 (0%)]\tLoss: 0.106069\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [256/5216 (5%)]\tLoss: 0.559406\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [512/5216 (10%)]\tLoss: 0.304897\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [768/5216 (15%)]\tLoss: 0.101011\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [1024/5216 (20%)]\tLoss: 0.022987\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [1280/5216 (25%)]\tLoss: 0.292488\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [1536/5216 (29%)]\tLoss: 0.068082\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [1792/5216 (34%)]\tLoss: 0.094365\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [2048/5216 (39%)]\tLoss: 0.260129\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [2304/5216 (44%)]\tLoss: 0.139533\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [2560/5216 (49%)]\tLoss: 0.081649\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [2816/5216 (54%)]\tLoss: 0.074408\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 4 [3072/5216 (59%)]\tLoss: 0.364739\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [3328/5216 (64%)]\tLoss: 0.090706\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [3584/5216 (69%)]\tLoss: 0.403308\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [3840/5216 (74%)]\tLoss: 0.055727\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [4096/5216 (79%)]\tLoss: 0.245138\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [4352/5216 (83%)]\tLoss: 0.002653\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 4 [4608/5216 (88%)]\tLoss: 0.078588\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 4 [4864/5216 (93%)]\tLoss: 0.165365\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [5120/5216 (98%)]\tLoss: 0.129308\n",
      "\n",
      "Epoch 4 Validation set: Average loss: 0.0073, Accuracy: 497/615 (81%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [0/5216 (0%)]\tLoss: 0.120243\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [256/5216 (5%)]\tLoss: 0.009768\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [512/5216 (10%)]\tLoss: 0.027487\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [768/5216 (15%)]\tLoss: 0.302586\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [1024/5216 (20%)]\tLoss: 0.212103\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [1280/5216 (25%)]\tLoss: 0.005566\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [1536/5216 (29%)]\tLoss: 0.400482\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [1792/5216 (34%)]\tLoss: 0.118808\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [2048/5216 (39%)]\tLoss: 0.105056\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [2304/5216 (44%)]\tLoss: 0.145817\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [2560/5216 (49%)]\tLoss: 0.067106\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [2816/5216 (54%)]\tLoss: 0.417104\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [3072/5216 (59%)]\tLoss: 0.430606\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [3328/5216 (64%)]\tLoss: 0.054372\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [3584/5216 (69%)]\tLoss: 0.254572\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [3840/5216 (74%)]\tLoss: 0.172261\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [4096/5216 (79%)]\tLoss: 0.008627\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [4352/5216 (83%)]\tLoss: 0.512159\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [4608/5216 (88%)]\tLoss: 0.110253\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [4864/5216 (93%)]\tLoss: 0.224685\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [5120/5216 (98%)]\tLoss: 0.008852\n",
      "\n",
      "Epoch 5 Validation set: Average loss: 0.0020, Accuracy: 473/615 (77%)\n",
      "\n",
      "Running Infected Model\n",
      "Net(\n",
      "  (fc1): Linear(in_features=663552, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "model is training\n",
      "Accuracy: 0.25\n",
      "Train Epoch: 1 [0/3875 (0%)]\tLoss: 0.739549\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [256/3875 (7%)]\tLoss: 0.446464\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 1 [512/3875 (13%)]\tLoss: 1.551532\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [768/3875 (20%)]\tLoss: 1.186277\n",
      "Accuracy: 0.4375\n",
      "Train Epoch: 1 [1024/3875 (26%)]\tLoss: 1.131077\n",
      "Accuracy: 0.3125\n",
      "Train Epoch: 1 [1280/3875 (33%)]\tLoss: 1.607352\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [1536/3875 (40%)]\tLoss: 1.555986\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 1 [1792/3875 (46%)]\tLoss: 1.546682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [2048/3875 (53%)]\tLoss: 0.556354\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 1 [2304/3875 (59%)]\tLoss: 0.191511\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 1 [2560/3875 (66%)]\tLoss: 0.754885\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [2816/3875 (72%)]\tLoss: 0.528874\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [3072/3875 (79%)]\tLoss: 0.731795\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [3328/3875 (86%)]\tLoss: 0.492734\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [3584/3875 (92%)]\tLoss: 0.434571\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 1 [3840/3875 (99%)]\tLoss: 1.042656\n",
      "\n",
      "Epoch 1 Validation set: Average loss: 0.0151, Accuracy: 332/381 (87%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 2 [0/3875 (0%)]\tLoss: 0.280990\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [256/3875 (7%)]\tLoss: 0.164306\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [512/3875 (13%)]\tLoss: 0.719353\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [768/3875 (20%)]\tLoss: 0.882683\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [1024/3875 (26%)]\tLoss: 0.420361\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [1280/3875 (33%)]\tLoss: 1.320678\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [1536/3875 (40%)]\tLoss: 0.549897\n",
      "Accuracy: 0.5\n",
      "Train Epoch: 2 [1792/3875 (46%)]\tLoss: 0.673739\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [2048/3875 (53%)]\tLoss: 0.188937\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 2 [2304/3875 (59%)]\tLoss: 2.254981\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [2560/3875 (66%)]\tLoss: 0.462911\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 2 [2816/3875 (72%)]\tLoss: 0.462193\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [3072/3875 (79%)]\tLoss: 1.072827\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [3328/3875 (86%)]\tLoss: 0.627748\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [3584/3875 (92%)]\tLoss: 0.736331\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [3840/3875 (99%)]\tLoss: 0.544788\n",
      "\n",
      "Epoch 2 Validation set: Average loss: 0.0091, Accuracy: 248/381 (65%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [0/3875 (0%)]\tLoss: 0.521022\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 3 [256/3875 (7%)]\tLoss: 0.814769\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [512/3875 (13%)]\tLoss: 0.834503\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [768/3875 (20%)]\tLoss: 0.758412\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 3 [1024/3875 (26%)]\tLoss: 0.511728\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [1280/3875 (33%)]\tLoss: 0.443034\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [1536/3875 (40%)]\tLoss: 0.438565\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [1792/3875 (46%)]\tLoss: 0.599062\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 3 [2048/3875 (53%)]\tLoss: 0.771730\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [2304/3875 (59%)]\tLoss: 0.255860\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [2560/3875 (66%)]\tLoss: 0.579139\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [2816/3875 (72%)]\tLoss: 0.797824\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [3072/3875 (79%)]\tLoss: 0.493315\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [3328/3875 (86%)]\tLoss: 0.448299\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [3584/3875 (92%)]\tLoss: 0.579763\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [3840/3875 (99%)]\tLoss: 0.464177\n",
      "\n",
      "Epoch 3 Validation set: Average loss: 0.0046, Accuracy: 337/381 (88%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 4 [0/3875 (0%)]\tLoss: 0.575392\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 4 [256/3875 (7%)]\tLoss: 0.633952\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [512/3875 (13%)]\tLoss: 0.364550\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [768/3875 (20%)]\tLoss: 0.404323\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [1024/3875 (26%)]\tLoss: 0.494458\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 4 [1280/3875 (33%)]\tLoss: 0.822179\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 4 [1536/3875 (40%)]\tLoss: 0.701630\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 4 [1792/3875 (46%)]\tLoss: 0.900565\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [2048/3875 (53%)]\tLoss: 0.538931\n",
      "Accuracy: 0.4375\n",
      "Train Epoch: 4 [2304/3875 (59%)]\tLoss: 0.972637\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 4 [2560/3875 (66%)]\tLoss: 0.713736\n",
      "Accuracy: 0.4375\n",
      "Train Epoch: 4 [2816/3875 (72%)]\tLoss: 0.952438\n",
      "Accuracy: 0.375\n",
      "Train Epoch: 4 [3072/3875 (79%)]\tLoss: 1.708384\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [3328/3875 (86%)]\tLoss: 0.644007\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [3584/3875 (92%)]\tLoss: 0.605869\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [3840/3875 (99%)]\tLoss: 0.700652\n",
      "\n",
      "Epoch 4 Validation set: Average loss: 0.0132, Accuracy: 325/381 (85%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 5 [0/3875 (0%)]\tLoss: 0.430820\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 5 [256/3875 (7%)]\tLoss: 0.462215\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [512/3875 (13%)]\tLoss: 0.385881\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [768/3875 (20%)]\tLoss: 0.457408\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [1024/3875 (26%)]\tLoss: 0.379391\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [1280/3875 (33%)]\tLoss: 0.514620\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [1536/3875 (40%)]\tLoss: 0.401033\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [1792/3875 (46%)]\tLoss: 0.317281\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 5 [2048/3875 (53%)]\tLoss: 0.660884\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 5 [2304/3875 (59%)]\tLoss: 0.558020\n",
      "Accuracy: 0.5\n",
      "Train Epoch: 5 [2560/3875 (66%)]\tLoss: 0.843729\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [2816/3875 (72%)]\tLoss: 0.280389\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 5 [3072/3875 (79%)]\tLoss: 0.751605\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [3328/3875 (86%)]\tLoss: 0.320877\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [3584/3875 (92%)]\tLoss: 0.341283\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [3840/3875 (99%)]\tLoss: 0.314077\n",
      "\n",
      "Epoch 5 Validation set: Average loss: 0.0108, Accuracy: 335/381 (88%)\n",
      "\n",
      "Running models with 4 convolutional layers\n",
      "==============================\n",
      "\n",
      "Running Initial Model\n",
      "Net(\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=1290496, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "model is training\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 1 [0/5216 (0%)]\tLoss: 0.686432\n",
      "Accuracy: 0.5\n",
      "Train Epoch: 1 [256/5216 (5%)]\tLoss: 14.732996\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [512/5216 (10%)]\tLoss: 1.910650\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [768/5216 (15%)]\tLoss: 0.513288\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [1024/5216 (20%)]\tLoss: 0.540658\n",
      "Accuracy: 0.5\n",
      "Train Epoch: 1 [1280/5216 (25%)]\tLoss: 0.858416\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [1536/5216 (29%)]\tLoss: 0.414726\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [1792/5216 (34%)]\tLoss: 0.576206\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [2048/5216 (39%)]\tLoss: 0.418616\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [2304/5216 (44%)]\tLoss: 0.512112\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [2560/5216 (49%)]\tLoss: 0.563691\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [2816/5216 (54%)]\tLoss: 0.575560\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 1 [3072/5216 (59%)]\tLoss: 0.608299\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [3328/5216 (64%)]\tLoss: 0.374221\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 1 [3584/5216 (69%)]\tLoss: 0.330065\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [3840/5216 (74%)]\tLoss: 0.675095\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 1 [4096/5216 (79%)]\tLoss: 0.708460\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 1 [4352/5216 (83%)]\tLoss: 0.587207\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [4608/5216 (88%)]\tLoss: 0.633278\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [4864/5216 (93%)]\tLoss: 0.528364\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 1 [5120/5216 (98%)]\tLoss: 0.645096\n",
      "\n",
      "Epoch 1 Validation set: Average loss: 0.0091, Accuracy: 381/615 (62%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [0/5216 (0%)]\tLoss: 0.737498\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [256/5216 (5%)]\tLoss: 0.553129\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [512/5216 (10%)]\tLoss: 0.618779\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [768/5216 (15%)]\tLoss: 0.710895\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [1024/5216 (20%)]\tLoss: 0.550072\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [1280/5216 (25%)]\tLoss: 0.683108\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [1536/5216 (29%)]\tLoss: 0.611891\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [1792/5216 (34%)]\tLoss: 0.420706\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [2048/5216 (39%)]\tLoss: 0.513383\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [2304/5216 (44%)]\tLoss: 0.602414\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [2560/5216 (49%)]\tLoss: 0.549083\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 2 [2816/5216 (54%)]\tLoss: 0.515360\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 2 [3072/5216 (59%)]\tLoss: 0.538889\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [3328/5216 (64%)]\tLoss: 0.628293\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [3584/5216 (69%)]\tLoss: 0.662578\n",
      "Accuracy: 0.4375\n",
      "Train Epoch: 2 [3840/5216 (74%)]\tLoss: 0.849126\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [4096/5216 (79%)]\tLoss: 0.573539\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 2 [4352/5216 (83%)]\tLoss: 0.598035\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 2 [4608/5216 (88%)]\tLoss: 0.667767\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 2 [4864/5216 (93%)]\tLoss: 0.394146\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 2 [5120/5216 (98%)]\tLoss: 0.528087\n",
      "\n",
      "Epoch 2 Validation set: Average loss: 0.0080, Accuracy: 381/615 (62%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/5216 (0%)]\tLoss: 0.493609\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [256/5216 (5%)]\tLoss: 0.508908\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [512/5216 (10%)]\tLoss: 0.354097\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [768/5216 (15%)]\tLoss: 0.487352\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [1024/5216 (20%)]\tLoss: 0.404245\n",
      "Accuracy: 0.5625\n",
      "Train Epoch: 3 [1280/5216 (25%)]\tLoss: 0.618143\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [1536/5216 (29%)]\tLoss: 0.435599\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [1792/5216 (34%)]\tLoss: 0.341731\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 3 [2048/5216 (39%)]\tLoss: 0.456525\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [2304/5216 (44%)]\tLoss: 0.427763\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [2560/5216 (49%)]\tLoss: 0.457158\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [2816/5216 (54%)]\tLoss: 0.409349\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [3072/5216 (59%)]\tLoss: 0.509116\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [3328/5216 (64%)]\tLoss: 0.480765\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [3584/5216 (69%)]\tLoss: 0.348157\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 3 [3840/5216 (74%)]\tLoss: 0.485969\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 3 [4096/5216 (79%)]\tLoss: 0.175340\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [4352/5216 (83%)]\tLoss: 0.406742\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 3 [4608/5216 (88%)]\tLoss: 0.556554\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 3 [4864/5216 (93%)]\tLoss: 0.332751\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 3 [5120/5216 (98%)]\tLoss: 0.484372\n",
      "\n",
      "Epoch 3 Validation set: Average loss: 0.0189, Accuracy: 382/615 (62%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [0/5216 (0%)]\tLoss: 0.250474\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [256/5216 (5%)]\tLoss: 0.357249\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 4 [512/5216 (10%)]\tLoss: 0.501182\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [768/5216 (15%)]\tLoss: 0.236452\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 4 [1024/5216 (20%)]\tLoss: 0.238788\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 4 [1280/5216 (25%)]\tLoss: 0.553861\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 4 [1536/5216 (29%)]\tLoss: 0.442150\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [1792/5216 (34%)]\tLoss: 0.392263\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [2048/5216 (39%)]\tLoss: 0.156812\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 4 [2304/5216 (44%)]\tLoss: 0.279392\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [2560/5216 (49%)]\tLoss: 0.385522\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 4 [2816/5216 (54%)]\tLoss: 0.265210\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [3072/5216 (59%)]\tLoss: 0.412495\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [3328/5216 (64%)]\tLoss: 0.451064\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [3584/5216 (69%)]\tLoss: 0.477053\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 4 [3840/5216 (74%)]\tLoss: 0.321481\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 4 [4096/5216 (79%)]\tLoss: 0.234261\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [4352/5216 (83%)]\tLoss: 0.292274\n",
      "Accuracy: 0.625\n",
      "Train Epoch: 4 [4608/5216 (88%)]\tLoss: 0.578541\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 4 [4864/5216 (93%)]\tLoss: 0.300700\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 4 [5120/5216 (98%)]\tLoss: 0.389771\n",
      "\n",
      "Epoch 4 Validation set: Average loss: 0.0090, Accuracy: 468/615 (76%)\n",
      "\n",
      "model is training\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [0/5216 (0%)]\tLoss: 0.330089\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [256/5216 (5%)]\tLoss: 0.472691\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [512/5216 (10%)]\tLoss: 0.409767\n",
      "Accuracy: 0.6875\n",
      "Train Epoch: 5 [768/5216 (15%)]\tLoss: 0.402484\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [1024/5216 (20%)]\tLoss: 0.168471\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [1280/5216 (25%)]\tLoss: 0.267961\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [1536/5216 (29%)]\tLoss: 0.272690\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [1792/5216 (34%)]\tLoss: 0.718050\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [2048/5216 (39%)]\tLoss: 0.271338\n",
      "Accuracy: 0.8125\n",
      "Train Epoch: 5 [2304/5216 (44%)]\tLoss: 0.408446\n",
      "Accuracy: 0.75\n",
      "Train Epoch: 5 [2560/5216 (49%)]\tLoss: 0.326411\n",
      "Accuracy: 0.9375\n",
      "Train Epoch: 5 [2816/5216 (54%)]\tLoss: 0.129955\n",
      "Accuracy: 1.0\n",
      "Train Epoch: 5 [3072/5216 (59%)]\tLoss: 0.186322\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [3328/5216 (64%)]\tLoss: 0.384327\n",
      "Accuracy: 0.875\n",
      "Train Epoch: 5 [3584/5216 (69%)]\tLoss: 0.296891\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters\n",
    "hidden_units = 1024\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "model_name = \"custom\"\n",
    "save_dir = False\n",
    "\n",
    "for layer in range(3,8):\n",
    "    print(\"Running models with {} convolutional layers\".format(layer))\n",
    "    print(\"==============================\\n\")\n",
    "    print(\"Running Initial Model\")\n",
    "    initial_model = healthy_model(n_hidden=[hidden_units], n_epoch=epochs, labelsdict=initial_classes, lr=learning_rate, device=device, \\\n",
    "                    model_name=model_name, trainloader=first_train_loader, validloader=first_test_loader, train_data=ld_train, layers=layer)\n",
    "    print(\"Running Infected Model\")\n",
    "    infected_model = covid_model(n_hidden=[hidden_units], n_epoch=epochs, labelsdict=infected_classes, lr=learning_rate, device=device, \\\n",
    "                    model_name=model_name, trainloader=infected_train_loader, validloader=infected_test_loader, train_data=ld_train_infected, layers=layer)\n",
    "\n",
    "if save_dir:\n",
    "    save_checkpoint(model, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "loss_vals = load('health.npy')\n",
    "print(loss_vals)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(20)\n",
    "print(loss_vals.shape)\n",
    "p = np.arange(1,loss_vals.shape[0]+1,1)\n",
    "plt.plot(p,loss_vals,linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "loss_vals = load('covid.npy')\n",
    "print(loss_vals)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(20)\n",
    "print(loss_vals.shape)\n",
    "p = np.arange(1,loss_vals.shape[0]+1,1)\n",
    "plt.plot(p,loss_vals,linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def validationTest(modelhealthy, modelcovid, device, valid_loader):\n",
    "    modelhealthy.eval()\n",
    "    modelcovid.eval()\n",
    "    valid_loss = 0\n",
    "    TP1 = 0\n",
    "    TP2 = 0\n",
    "    TN1 = 0\n",
    "    TN2 = 0\n",
    "    FP1 = 0\n",
    "    FP2 = 0\n",
    "    FN1 = 0\n",
    "    FN2 = 0\n",
    "    label_list = []\n",
    "    pre_list = []\n",
    "    print_list = []\n",
    "    final_output = torch.Tensor([])\n",
    "    healthy_img = []\n",
    "    covid_img = []\n",
    "    no_covid_img = []\n",
    "    with torch.no_grad():\n",
    "        # Loop over all examples in test set\n",
    "        for image, label in test_loader:\n",
    "\n",
    "            # Send the data and label to the device\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # Pass the image through the model\n",
    "            output = modelhealthy(image)\n",
    "            # Get the index of the max log-probability\n",
    "            init_pred = output.max(1, keepdim = True)[1]\n",
    "            print(init_pred)\n",
    "\n",
    "            if init_pred == 1:\n",
    "                output2 = modelcovid(image)\n",
    "                init_pred = output.max(1,keepdim=True)[1]\n",
    "            image_ex = image.squeeze().detach().cpu().numpy()\n",
    "            pre_list.append(init_pred)\n",
    "            label_list.append(target2.view_as(pred2))\n",
    "            print_list.append([image_ex,label,init_pred])\n",
    "        print(multilabel_confusion_matrix(label_list, pre_list, labels=[0,1,2]))\n",
    "    \n",
    "    return print_list\n",
    "\n",
    "printlist = validationTest(initial_model, infected_model, device, ld_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.suptitle('Validation set images with target and predictions \\n accuracy{},TN{},FN{}'.format('remembertosetthis',2,3), fontsize=10)\n",
    "# Browse through epsilon values and adversarial examples\n",
    "\n",
    "for j in range(len(printlist)):\n",
    "    cnt += 1\n",
    "    plt.subplot(6, 4, cnt)\n",
    "\n",
    "    # Remove x-axis and y-axis ticks from plot\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    # set title for plot\n",
    "\n",
    "    # Labels for each image subplot\n",
    "    picture, target, pred = printlist[j]\n",
    "    plt.title(\"Ground Truth Label{} -> Predicted Label{}\".format(target, pred))\n",
    "\n",
    "    # Display image\n",
    "    plt.imshow(picture)\n",
    "        \n",
    "# Display full plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
